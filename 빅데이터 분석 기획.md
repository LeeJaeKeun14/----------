# 빅데이터 분석 기획

- [빅데이터 분석 기획](#빅데이터-분석-기획)
  - [빅데이터의 이해](#빅데이터의-이해)
    - [빅데이터 개요 및 활용](#빅데이터-개요-및-활용)
      - [데이터와 정보](#데이터와-정보)
      - [데이터베이스](#데이터베이스)
      - [빅데이터 개요](#빅데이터-개요)
      - [빅데이터 가치](#빅데이터-가치)
      - [데이터 산업의 이해](#데이터-산업의-이해)
      - [빅데이터 조직 및 인력](#빅데이터-조직-및-인력)
    - [빅데이터 기술 및 제도](#빅데이터-기술-및-제도)
      - [빅데이터 플랫폼](#빅데이터-플랫폼)
      - [빅데이터 처리기술](#빅데이터-처리기술)
      - [빅데이터와 인공지능](#빅데이터와-인공지능)
      - [개인정보 개요](#개인정보-개요)
      - [개인정보 법 제도](#개인정보-법-제도)
      - [개인정보 비식별화](#개인정보-비식별화)
      - [개인정보 활용](#개인정보-활용)
  - [데이터 분석 계획](#데이터-분석-계획)
    - [분석 방안 수립](#분석-방안-수립)
      - [데이터 분석](#데이터-분석)
      - [데이터 분석 기획](#데이터-분석-기획)
      - [분석 마스터 플랜과 로드맵 설정](#분석-마스터-플랜과-로드맵-설정)
      - [분석 문제 정의](#분석-문제-정의)
      - [데이터 분석 방안](#데이터-분석-방안)
      - [빅데이터 분석 방법론](#빅데이터-분석-방법론)
      - [데이터 분석 거버넌스](#데이터-분석-거버넌스)
      - [데이터 분석 수준 진단](#데이터-분석-수준-진단)
    - [분석 작업 계획](#분석-작업-계획)
      - [분석 작업 개요](#분석-작업-개요)
      - [데이터 확보 계획](#데이터-확보-계획)
      - [분석 절차와 작업 계획](#분석-절차와-작업-계획)
      - [분석 프로젝트 관리](#분석-프로젝트-관리)
  - [데이터 수집 및 저장 계획](#데이터-수집-및-저장-계획)
    - [데이터 수집 및 전환](#데이터-수집-및-전환)
      - [데이터 수집](#데이터-수집)
      - [데이터 유형 및 속성 파악](#데이터-유형-및-속성-파악)
      - [데이터 변환](#데이터-변환)
      - [데이터 비식별화](#데이터-비식별화)
      - [데이터 품질 검증](#데이터-품질-검증)
    - [데이터 적재 및 저장](#데이터-적재-및-저장)
      - [데이터 적재](#데이터-적재)
      - [데이터 저장](#데이터-저장)

## 빅데이터의 이해

### 빅데이터 개요 및 활용

#### 데이터와 정보

- 데이터 정의 : 추론과 추정의 근거를 이루는 사실
- 데이터 특징
- 데이터의 구분 : 정량적, 정성정
- 데이터의 유형 : 정형, 반정형, 비정형
- 데이터의 근원에 따른 분류 : 가역, 불가역
- 데이터의 기능 : 암묵지, 형식지
- 지식창조 매커니즘
- 데이터 정보 지식 정제

#### 데이터베이스

- 데이터베이스의 정의
- 데이터베이스 관리 시스템 DBMS
  - SQL
- 데이터베이스의 특징
- 데이터베이스의 활용
  - OLTP, OLAP
- 데이터 웨어하우스
  - 주제지향, 통합, 시계열, 비휘발
  - 데이터모델, ETL, ODS, DW 메타데이터, OLAP,데이터마이닝, 분석도구, 경영기반 솔루션

#### 빅데이터 개요

- 빅데이터 등장과 변화
  - 데이터변화(3V), 기술변화, 인재 조직 변화
- 빅데이터의 등장으로 인한 변화
  - 사전 -> 사후, 질 -> 양, 인과관계 -> 상관관계
- 빅데이터의 특징
  - 규모, 유형, 속도(3V), 품질, 가치(+2V)
- 빅데이터의 활용
  - 자원, 기술, 인력

#### 빅데이터 가치

- 빅데이터의 기능과 효과
- 빅데이터 가치 측정의 어려움
- 빅데이터의 영향

#### 데이터 산업의 이해

- 데이터 산업의 진화
  - 데이터 처리 -> 통합 -> 분석 -> 연결 -> 권리
  - 데이터 처리시대 -> 통합 시대 -> 분석 시대 -> 연결 시대 -> 권리 시대
- 데이터 산업의 구조
  - 인프라, 서비스

#### 빅데이터 조직 및 인력

- 필요성
- 조직의 역활
- 조직의 구성
  - 집중형, 기능형, 분산형
- 데이터 사이언스 역량
  - 분석적, 데이터처리 관련 IT, 비즈니스 컨설팅
- 데이터 사이언티스트
  - Hard_Skill : 빅데이터 이론 지식, 분석 기술 숙련
  - Soft_Skill : 통찰력 있는 분석, 설득력 있는 전달, 커뮤니케이션

### 빅데이터 기술 및 제도

#### 빅데이터 플랫폼

- 빅데이터 플랫폼의 등장 배경
  - 비즈니스 요구 변화, 데이터 규모_처리 복잡도 증가, 데이터 구조 변화_신속성 요구, 데이터 분석 유연성 증대
- 빅데이터 플랫폼의 기능
  - 컴퓨팅 부하, 저장 부하, 네트워크 부하
- 빅데이터 플랫폼의 조건
  - 서비스 사용자 측면, 서비스 제공자 측면
- 빅데이터 플랫폼 구조
  - 소프트웨어 계층 ()
  - 플랫폼 계층 ()
  - 인프라스트럭처 계층 ()

#### 빅데이터 처리기술

- 빅데이터 처리과정과 요소기술
  - 생성->수집->저장(공유)->처리->분석->시각화
- 빅데이터 수집
  - 크롤링,...,ETL 프로세스
- 빅데이터저장
  - NoSQL, ..., 분산파일시스템 (HDFS, GFS, S3)
- 빅데이터 처리
  - 분산, 병렬 시스템
  - 분산 병렬 컴퓨팅
  - Hadoop, Apache Spark, MapReduce
  - 맵리듀스
    - Split -> Map -> Shuffle -> Reduce
- 빅데이터 분석
  - 탐구요인(EFA), 확인요인(CFA)

#### 빅데이터와 인공지능

- 인공지능
  - 기계학습 응용
    - 지도 (분류 / 회귀) : 이미지, 음성, 사기검증, 불량 / 시세, 가격 예측, 강우량 예측
    - 비지도 (군집 / 오토인코더 / GAN) : 텍스트 토픽, 고객 세그맨테이션 / 이상징후 탐지, 노이즈 제거, 벡터화/ 시뮬레이션, 누락 데이터 생성, 패션 데이터 생성
    - 강화 : 게임 플레이, 로폿 학습, 공급망 최적화
- 인공지능 데이터 학습의 진화
  - 전이학습
  - BERT
- 빅데이터와 인공지능 관계
  - 학습 데이터의 에노테이션
- 인공지능 기술동향
  - 기계학습 프레임워크 : Tensorflow, Keras
  - GAN
  - Auto-encoder
  - XAI
  - AutoML
- 인공지능의 한계점과 발전 방향
  - 국내 한계, 인공지능 미래

#### 개인정보 개요

- 개인정보의 정의와 판단기준
  - 살아 있는 개인에 관한 정보, 개인을 알아볼 수 있는 정보
- 개인정보 처리와 활용
  - 개인정보 이전, 개인정보 처리 위탁, 개인정보의 제3자 제공
- 개인정보 보호
  - 개인정보의 보호조치
  - 빅데이터 개인정보보호 가이드라인
    - 비식별화, 투명성확보, 재식별 시 조치, 민감정보 및 비밀정보 처리, 기술적 관리적 보호조치

#### 개인정보 법 제도

- 개인정보보호법
  - 당사자의 동의 없는 개인정보 수집, 제3자에게 제공하는 것 금지
- 정보통신망 이용촉진 및 정보보호 등에 관한 법률(정보통신망법)
  - 특별법 > 일반법, 특별법에 해당되지 않을경우 일밥법 적용
  - 정보통신망(특별법), 개인정보보호(일반법)
- 신용정보의 이용 및 보호에 관한 법률(신용정보보호법) (준특별)
- 2020년 데이터 3법의 주요 개정 내용
  - 데이터 이용 활성화를 위한 '가명정보'개념 도입. 데이터간 결합 근거 마련
  - 개인정보보호 관련 볍률의 유사_중복 규정을 정비
  - 데이터 활용에 따른 개인정보 책임 강화
  - 모호했던 개인정보 판단 기준 명확화

#### 개인정보 비식별화

- 개인정보 비식별화 개요
  - 비실별 정보, 비식별 조치, 비식별 정보의 활용, 비식별 정보의 보호
- 개인정보 비식별화 조치 가이드라인
  - 사전검토 -> 비식별조치 -> 적성성 평가 -> 사후관리
  - 가명처리, 총계처리, 데이터 삭제, 데이터 범주화, 데이터 마스킹

#### 개인정보 활용

- 데이터 수집의 위기 요인과 통제 방안
  - 사생활 침해로 위기 발생 : M2M
  - 동의에서 책임으로 강화하여 통제
- 데이터 활용의 위기 요인과 통제 방안
  - 책임원칙 훼손, 결과 기반 책임 원칙을 고수하여 통제
- 데이터 처리의 위기 요인과 통제 방안
  - 데이터 오용, 알고리즘 접근 허용하여 통제

## 데이터 분석 계획

### 분석 방안 수립

#### 데이터 분석

- 데이터 분석의 현황
- 데이터 분석의 지향점
- 데이터 분석에 대한 회의론
- 데이터 분석시 고려사항

#### 데이터 분석 기획

- 데이터 분석 기획 정의
  - 어떠한 목표(What)를 달성하기 위해(Why) 어떠한 데이터를 가지고 어떤 방식으로(How) 수행할 것인가에 대한 계획
- 분석 기획의 특징
  - 분석의 방식(How), 분석의 대상(What)
  - 1: Optimization(K, K), 2: Insight(k, uk)
  - 3: Solution(uk, k), Discovery(uk, uk)
- 분석 기획 시 필요 역략
- 분석 기획 시 고려사항

#### 분석 마스터 플랜과 로드맵 설정

- 분석 마스터 플랜
  - 분석 과제를 수행함에 있어 그 과제의 목적이나 목표에 따라 전체적인 방향성을 제시하는 기본계획
  - 정보전략계획(ISP)
- 분석 과제 우선순위 평가기준
  - 전략적 중요도(필요성, 시급성), 실행 용이성(투자, 기술)
  - ROI 투자 수익률
    - 투자비용 요소(데이터 크기, 형태, 속도), 비즈니스 효과(새로운 가치)
- 분석과제 우선순위 선정 및 조정
  - 시급성 : 전략성 중요도, 목표 가치(KPI)
  - 난이도 : 데이터 획득, 가공, 저장 비용, 분석 적용 비용, 수준
  - 1: 어려움, 긴급 2: 어려움, 미래
  - 3: 쉬움, 긴급 4: 쉬움, 미래
  - 3->(1,4)->2
- 분석 로드맵 설정
  - 데이터 분석체계 도입 -> 분석 유효성 검증 -> 분석 확산 및 고도화

#### 분석 문제 정의

- 분석 문제 정의 개요
  - 하양식 접근 : 문제가 먼저 주어지고 이에 해결 찾기
  - 상향식 접근 : 데이터 기반으로 문제의 재정 및 해결방안 탐색
  - 혼합방식
    - 상향, 발산(Diverge) : 가능한 옵션 도출
    - 하향, 수렴(Converge) : 도출된 옵션을 분석, 검증
- 하향식 접근 방식
  - Problem Discovery -> Problem Definition -> Solution Search -> Feasibility Study
  - 문제 탐색 :
  - 문제 정의 :
  - 해결방안 탐색 : 분석 기법 및 시스템(How), 분석 역량(Why)
    - 1: 기존 시스템 개선, 2: 교육 및 채용을 통한 역량 향상
    - 3: 시스템 고도화, 4: 전문업체 솔루션
- 하향식 접근 방식의 문제 탐색 방법
  - 업무 단위, 제품 단위, 고객 단위, 규제와 감시 영역, 지원 인프라 영역
- 상향식 접근 방식
  - 데이터 분석을 통해 Why(왜) 그러한 일이 발생했는지 역으로 추적하면서 문제를 도출
  - What의 관점으로 접근
  - 프로토타이핑 접근법 : 일단 분석을 시도해 보고, 그 결과를 확인

#### 데이터 분석 방안

- 분석 방법론
  - 프로젝트는 한 개인의 역량이나 조직의 우연한 성공에 의해서는 안 되고 일정 품질 수준 이상의 산출물과 프로젝트 성공 가능성을 제시
  - 상세한 절차, 방법, 도구와 기법, 탬플릿과 산출물, 어느정도의 지식만 있으면 활용 가능한 수준의 난이도
  - 형식화, 체계화, 내재화
- 계층적 프로세스 모델 구성
  - 최상위 계층 - 단계 (Phase)
  - 중간 계층 - 테스크 (Task)
  - 최하위 계층 - 스텝 (Step)
- 소프트웨어 개발생명주기 활용
  - 계획-요구분석-설계-구현-시험-유지보수
  - 폭포수 모형 : 검토 및 승인을 거처 순차적, 하향식 개발
    - 이해 쉽다
    - 요구사항 도출 어렵, 코딩, 테스트 지연, 문제점 발견 느림
  - 프로토타입 모형 : 시스템의 핵심적인 기능을 먼저 만들어 평가 후 구현
    - 요구사항 도출과 시스템 이해 용이, 의사소통 향상
    - 폐기되는 프로토타입 발생, 사용자의 오해가 발생
  - 나선형모형 : 폴포수 모형과 프로토타입 모형의 장점에 위험분석 추가
    - 집중적으로 개발 시 실패 위험을 감소, 테스트 용이
    - 관리가 복잡하다
  - 반복적 모형
    - 시스템을 여러 번 나누어 릴리즈
- KDD 분석 방법론
  - 프로파일링기술 기반 데이터마이닝 프로세스
  - 데이터셋 선택
  - 데이터 전처리 : Noise, Outlier, Missing Value 제거
  - 데이터 변환 : 변수 선택, 차원 축소
  - 데이터 마이닝 : 알고리즘 선택, 데이터 전처리, 변환 추가 시행
  - 데이터 마이닝 결과 평가 : 결과 해석, 분석 목적과 일치성 확인
- CRISP-DM 분석 방법론
  - 계층적 프로세스 모델 4계층 구성
  - 최상위 레벨 : 여러 Phases 구성
  - 일반화 테스크 : 데이터 마이닝의 단일 프로세스 완전하게 수행
  - 세분화 테스크 : 일반화 테스크 구체적 수행
  - 프로세스 실행 : 데이터 마이닝 구체적 실행
  - 분석 절차
    - 업무 이해
    - 데이터 이해
    - 데이터 준비
    - 모델링
    - 평가
    - 전개
  - KDD 방법론에서 업무 이해와 전개가 추가
- SEMMA(Sample, Explore, Modify, Model and Assess)
  - 추출
  - 탐색
  - 수정
  - 모델링
  - 평가

#### 빅데이터 분석 방법론

- 빅데이터 분석 방법론 개요
  - 단계(Phase), 태스크(Task), 스텝(Step)
- 빅데이터 분석 방법론의 개발 절차
  - 분석 기획 :
    - 비즈니스 이해
    - 프로젝트 정의
    - 프로젝트 위험 계획
  - 데이터 준비 :
    - 필요 데이터 정의
    - 데이터 스토어 설계
    - 데이터 수집 및 정합성 점검
  - 데이터 분석 :
    - 분석용 데이터 준비
    - 텍스트 분석
    - 탐색석 분석
    - 모델링
    - 모델 평가 및 검증
  - 시스템 구현 :
    - 설계 및 구현
    - 시스템 테스트 및 운영
  - 평가 및 전개 :
    - 모델 발전계획 수립
    - 프로젝트 평가 및 보고

#### 데이터 분석 거버넌스

- 데이터 분석 거버넌스 개요
  - 데이터 분석 업무를 하나의 기업 문화로 정착하고 이를 지속적으로 고도화 하기 위해 필요
  - 데이터 분석 거버넌스의 구성요소
  - Organization
  - Process
  - System
  - Data
  - Human Resource
- 데이터 분석 기획과 관리를 수행하는 조직
  - part1_Chapter1_Section1 빅데이터 조직 및 인력
- 데이터 분석 과제 기획과 운영 프로세스
  - 과제 발굴단계, 과제 수행 및 모니터링 단계
  - 분석 idea 발굴->분석과제 후보제안->분석과제 확정->팀 구성->분석과제 실행->분석과제 진행 관리->결과 공유 개선
- 데이터 분석 지원 인프라
  - 개발 시스템, 플랫폼 구조
- 데이터 거버넌스
  - 필요성
  - 정의
  - 주요 관리 대상 : 마스터 데이터, 메타 데이터, 데이터 사전
  - 특징
  - 빅데이터 거버넌스 특징
  - 구성 요소 : 원칙(Principle), 조직(Organization), 프로세스(Process)
  - 체계
    - 데이터 표준화
    - 데이터 관리 체계
    - 데이터 저장소 관리
    - 표준화 행동
- 데이터 분석 교육 및 마인드 육성 체계
  - 필요성
  - 데이터 분석 문화 도입방안
    - 준비기 -> 도입기 -> 안정 추진기
  - 적극적 도입방안
  - 데이터 분석 교육방안
  - 데이터 분석 방법 및 분석적 사고 교육

#### 데이터 분석 수준 진단

- 데이터 분석 수준진단 개요
  - 필요성
  - 목표
  - 프레임워크
    - 6개의 영역의 분석 준비도와 3개의 영역 분석 성숙도를 동시 평가
    - 분석 업무, 분석 인력_조직, 분석 기법, 분석 데이터, 분석 문화, 분석 인프라
    - 비즈니스, 조직 및 역활, IT
- 분석 준비도
  - 정의
  - 원리 : 총 6가지 영역을 대상으로 현재 수준 파악
  - 데이터 분석 준비도 프레임워크
- 분석 성숙도 모델
  - 정의
  - 특징
  - 상세화
    - 도입단계, 활용단계, 확산 단계, 최적화 단계
    - 설명
    - 비즈니스
    - 조직_역량부문
    - IT부문
- 분석 수준진단 결과
  - 조직의 현재 데이터 분석 수준을 객관적으로 파악할 수 있다.
  - 사분면 분석
    - 준비도, 성숙도
    - 1(L, H): 정착형
    - 2(H, H): 확산형
    - 3(L, L): 준비형
    - 4(H, L): 도입형

### 분석 작업 계획

#### 분석 작업 개요

- 데이터 소스 -> 데이터 수집 -> 데이터 저장 -> 데이터 처리
- -> 데이터 분석 -> 데이터 표현

- 데이터 처리 영역
  - 데이터 소스 : 내부 데이터, 외부 데이터, 미디어 정보
  - 데이터 수집 : 입력, 로그 수집기, 크롤링, 센싱
  - 데이터 저장 : 정형 데이터, 비정형 데이터, 저장 장치
  - 데이터 처리 : 배치 처리, 실시간 처리, 분산 처리
- 데이터 분석 영역
  - 데이터 분석 NCS(꾹가직무능력표준) :
    - 전처리, 분석 방법, 머신러닝, 딥러닝
    - 도메인 이슈 도출, 분석목표 수립, 프로젝트 계획 수립, 보유 데이터 자산 확인
  - 데이터 표현 : 시각, 관계, 공간, 분포 시각화

#### 데이터 확보 계획

- 데이터 확보를 위한 사전 검토사항
  - 필요 데이터의 정의
  - 보유 데이터의 현황파악
  - 분석 데이터의 유형
  - 편향되지 않고 충분한 양의 데이터 규모
  - 내부 데이터의 사용
  - 외부 데이터 수집
- 분석에 필요한 변수 정의
  - 데이터 수집 기획
  - 분석 변수 정의
- 분석 변수 생성 프로세스 정의
  - 객관적 사실 기반의 문제 접근
  - 데이터의 상관 분석
  - 프로토타입을 통한 분석 변수 접근
- 생성된 분석 변수의 정제를 위한 점검항목 정의
  - 데이터 수집
  - 데이터 적합성
  - 특징변수
  - 타당성
- 생성된 분석 변수의 전처리 방법 수립
  - 데이터 전처리 수행
    - 데이터 정제
    - 데이터 통합
    - 데이터 축소
    - 데이터 변환
- 생성 변수의 검증 방안 수립
  - 분석 변수의 데이터 검증 방안 수립
    - 대량 데이터, 정밀 데이터, 데이터 출처 불명확
  - 정확성, 완정성, 적시성, 일관성

#### 분석 절차와 작업 계획

- 분석 절차
  - 분석 정차의 특징
  - 문제 인식, 연구조사, 모형화, 데이터 수집, 데이터 분석, 분석 결과 제시
    - 문제에 대한 구체적 정의가 없다면 데이터 마이닝 기반으로 데이터를 분석하여 인사이트를 발굴하거나 일단 데이터 분석을 시도한 후 결과를 확인해 가면서 반복적으로 개선 결과를 도출해 볼 수 있다.
- 작업 계획
  - 분석 작업 계획 수립
    - 프로젝트 소요비용 배분
    - 프로젝트 작업분할구조 수집
    - 프로젝트 업무 분장 계획 및 배분
  - 분석 작업 계획 수립을 위한 작업분할구조 WBS
    - 데이터 분석과제 정의
    - 데이터 준비 및 탐색
    - 데이터 분석 모델링 및 검증
    - 산출물 정리
- 분석목표정의서
  - 원천 데이터 조사
    - 데이터 정보
    - 데이터 수집 난이도
  - 성과평가 기준
    - 정성적 평가
    - 정량적 평가

#### 분석 프로젝트 관리

- 분석 프로젝트
  - 분석 프로젝트의 추가적 속성
    - 데이터 크기
    - 데이터 복잡도
    - 속도
    - 분석 모형의 복잡도
    - 정확도와 정밀도
  - 정확도와 정밀도의 관점
    - 낮은 정확도, 낮은 정밀도
    - 낮은 정확도, 높은 정밀도
    - 높은 정확도, 낮은 정밀도
    - 높은 정확도, 높은 정밀도
- 분석 프로젝트 관리
  - 효율적인 데이터 분석 수행을 위한 필요성
  - 분석 프로젝트의 관리 방안
- 분석 프로젝트의 영역별 주요 관리 항목
  - 범위 관리
  - 일정 관리
  - 원가 관리
  - 품질 관리
  - 통합 관리
  - 조달 관리
  - 인적자우너 관리
  - 위험 관리
  - 의사소통 관리
  - 이해관계자 관리

## 데이터 수집 및 저장 계획

### 데이터 수집 및 전환

#### 데이터 수집

- 데이터 수집
  - 데이터 수집 수행 자료
    - 용어집
    - 원천 데이터 소유 기관 정보
    - 서비스 흐름도
    - 데이터 수집 기술 매뉴얼
    - 업무 매뉴얼
    - 인프라 구성도
    - 데이터 명세서
    - 소프트웨어 아키텍처 개녕도
    - 데이터 수집 계획서
    - 수집 솔루션 매뉴얼
    - 원천 데이터 담당자 정보
    - 하둡 오퍼레이션 매뉴얼
    - 비즈니스 및 원천 데이터 파악을 위한 비즈니스 모델
  - 기초 데이터 수집 수행 절차
    - 비즈니스 도메인 정보 수집
    - 분석기획서 기반 도메인, 서비스 이해
    - 수집 데이터 탐색
    - 기초 데이터 수집
  - 데이터 수집 시스템 구축 절차
    - 수집 데이터 유형 파악
    - 수집 기술 결정
    - 아키텍처 수립
    - 하드웨어 구축
    - 실행환경 구축
- 비즈니스 도메인과 원천 데이터 정보 수집
  - 원천 데이터 정보
    - 데이터의 수집 가능성
    - 데이터의 보안
    - 데이터 정확성
    - 수집 난이도
    - 수집 비용
- 내 외부 데이터 수집
  - 내부 데이터 : 서비스 시스템, 네트워크 및 서버 정비, 마케팅 데이터
  - 외부 데이터 : 소설 데이터, 특정 기관 데이터, M2M 데이터, LOD
  - 데이터의 수집 방법 : 업무 협의, 수집 경로, 수집 대상
- 데이터 수집 기술

  - 데이터 유형별 데이터 수집 기술
    - 정형 데이터 : ETL, FTP, API, DBToDB, 스쿱
    - 비정형 데이터 : 크롤링, RSS, Open API, 척와(Chukwa), 카프카(Kafka)
    - 반정형 데이터 :
      - 풀럼(Flume)
      - 스크라이브(Scribe)
      - 센싱(Sencing)
      - 스트리밍(Streaming) : TCP, UDP, Bluetooth, RFID
  - ETL : 추출(Extract), 변환(Transform), 적재(Load) 프로세스
  - FTP : File Transfer Protocol
    - FTP는 대량의 파일을 네트워크를 통해 주고받을 때 사용되는 파일 전송 서비스이다.

  - 정형 데이터 수집을 위한 아파치 스쿱(Sqoop) 기술
    - 관계형 데이터 스토어 간에 대량 데이터를 효과적으로 전송하기 위해 구분된 도구이다
    - 커넥터를 사용하여 MySQL, Oracle, MS SQL 등 관계형 관계형 데이터베이스의 데이터를 하둡 파일시스템(HDFS, Five, Hbase)으로 수집한다
    - 데이터들을 하둡 맵리듀스(Mapreduce)로 변환하고, 변환된 데이터들을 다시 관계형 데이터베이스로 내보낼 수 있다.

  - 아파치 스쿱 특징
    - Bulk import 지원
    - 데이터 전송 병렬화
    - Direct input 제공
    - 프로그래밍 방식의 데이터 인터랙션

  - 로그/센서 데이터 수집을 위한 아파치 플럼(Flume)기술
    - 아파치 플럼은 대용량의 로그 데이터를 효과적으로 수집, 집계, 이동, 시키는 신뢰성 있는 분산 서비스를 제공하는 솔루션이다.
    - 신뢰성, 확장성, 효율성

  - 웹 및 소셜 데이터 수집을 위한 스크래피(Scrapy) 기술
    - 스크래피 특징 : 파이썬 기반의 프레임워크로 스크랩 과정이 단순하며 한 번에 여러 페이지를 불러오기 수월하다
    - 파이썬 기반, 단순한 스크랩 과정, 다양한 부가 요소

#### 데이터 유형 및 속성 파악

- 데이터 수집 세부 계획 작성
  - 데이터 유형, 위치, 크기, 보관방식, 수집주기, 확보비용, 데이터 이관 절차를 조사하여 세부 계획서를 작성한다.
- 데이터 위치 및 비용
  - 데이터의 종류
  - 데이터의 크기 및 보관주기
  - 데이터의 수집 주기
  - 데이터의 수집 방식
  - 데이터의 수집 기술
  - 데이터의 가치성
- 수집되는 데이터 형태
  - HTML, XML, JSON
- 데이터 저장 방식
  - 파일시스템
  - 관계형 데이터베이스
  - 분산처리 데이터베이스
- 데이터 적절성 검증
  - 데이터 누락 점겅
  - 소스 데이터와 비교
  - 데이터의 정확성 점검
  - 보안 사향 점검
  - 저작권 점검
  - 대량 트래픽 발생 여부

#### 데이터 변환

- 데이터 변환
  - 데이터 변환 방식의 종류
    - 관계형 DB(MySQL, Oracle, DB2, PostgreSQL) 비정형 데이터를 정형 데이터로
    - 분산데이터 저장(HDFS)
    - 데이터 웨어하우스(네티자, 테라데이터)(상용 라이센스) : 주제별, 시계열적으로 저장
    - NoSQL(Hbase, cassandra MongoDB) : 키-값 형태로 저장
  - 데이터 변환 수행자료
    - 데이터 수집 계획서, 수집 솔루션, 변환 솔루션
    - 하둡 오퍼레이션 메뉴얼, 소프트웨어 아키텍처 개념도
- 데이터베이스 구조 설계
  - DBMS 구축여부 결정
  - 저장 데이터베이스 결정
  - DBMS 설치
  - 테이블 구조 설계
- 비정형/반정형 데이터 변환
  - 비정형/반정형 데이터를 구조적 형태로 전환하여 저장
    - 수집데이터 구조 파악
    - 데이터 수집 절차에 대한 수행 코드 정리
    - 데이터 저장 프로그램 작성
    - 데이터베이스에 저장
- 융합 데이터베이스 설계
  - 요구사항 분석
  - 데이터 표준화와 모델링 수행
- 고려사항

#### 데이터 비식별화

- 비식별화 개요
  - 식별자(Identifier)와 속성자(Attribute value)
  - 식별자
    - 고유식별번호(주민번호, 여권, 운전면허 등)
    - 성명
    - 상세주소
    - 날자정보(생일, 돌, 결혼기념일, 환갑, 자격증 취득일)
    - 전화번호
    - 의료기록 번호
    - 계좌번호, 신용카드 번호
    - 자격증 번호
    - 자동차 번호, 기기 등록번호
    - 사진
    - 신체 식별정보(지문, 음성, 홍체)
    - 이메일 주소, ip, Mac, 홈페이지 URL
    - 식별코드
    - 기타
  - 속성자
    - 개인 특성 : 성별, 연령, 국적, 고향, 시군구, 우편번호, 병역, 결혼, 종교
    - 신체 특성 : 혁액형, 신장, 몸무게, 혀리둘레, 장애등급, 병명, 투약코드
    - 신용 특성 : 세금 납부액, 신용등급, 기부금, 건강보험료, 소득분위
    - 경력특성 : 학교, 학과, 학년, 성적, 경력, 직업, 직종, 직장명
    - 전자적 특성 : 쿠키정보, 접속일시, 방문일시, 인터넷 접속기록, 휴대전화 사용기록
    - 가족 특성 : 배우자, 자녀, 부모, 형제, 법적 대리인
  - 비식별 조치 방법
    - 가명처리 (Pseudonymization)
    - 총계처리 (Aggregation)
    - 데이터 삭제 (Data Reduction)
    - 데이터 범주화 (Data Suppression)
    - 데이터 마스킹 (Data Masking)
- 가명처리 Pseudonymization
  - 휴리스틱 가명화 Heuristic Pseudonymization
  - 암호화 Encryption
  - 교환 방법 Swapping
    - 장점 : 데이터 변형, 변질 수준이 적다
    - 단점 : 대체 값 부여 시에도 식별 가능한 고유 속성이 유지된다
- 총계처리 Aggregation
  - 부분총계 Micro Aggregation
  - 라운딩 Rounding
  - 재배열 Rearrangement
    - 장점 : 민감한 수치 정보에 비식별 조치가 가능하며, 통계 분석용 데이터셋 작성에 유리하다
    - 단점 : 정밀 분석이 어려우며, 집계 수량이 적을 경우 추론에 의한 식별 가능성이 있다
- 데이터 삭제 Data Reduction
  - 식별자 (부분)삭제
  - 레코드 삭제 Reducing Records
  - 식별요소 전부삭제
    - 장점 : 개인 식별요소의 전부 및 일부 삭제 처리가 가능하다
    - 단점 : 분석의 다양성과 결과의 유효성_신뢰성이 저하된다
- 데이터 범주화 Data Suppression
  - 감추기
  - 랜덤 라운딩 Random Rounding
  - 범위 방법 Data Range
  - 제어 라운딩 Controlled Rounding
    - 장점 : 통계형 데이터 형식이므로 다양한 분석 및 가공 가능
    - 단점 : 정확한 분석결과 도출이 어려우며, 범위 구간이 좁혀질 경우 추론이 가능하다
- 데이터 마스킹 Data Masking
  - 임의 잡음 추가 Adding Random Noise
  - 공백(blank)와 대체(impute)
    - 장점 : 개인 식별 요소를 제거하는 것이 가능하며, 원 데이터 구조에 대한 변형이 적다
    - 단점 : 마스킹을 과도하게 적용할 경우 데이터 필요 목적에 활용하기 어려우며 마스킹 수준이 낮을경우 특정한 값에 대한 추론 가능
- 적정성 평가
  - k-익명성 (k개 이상으로 해야 식별 가능)
    - 동질성 공격 : 범주화된 데이터를 연결하여 정보 추출
    - 배경지식 공격 : 성별에 따른 질병 등 배경지식을 활용하여 특정 정보(성별)을 유추할 수 있는 공격
  - l-다양성 (민감 데이터를 l개 이상으로 다양성을 가지도록 한다)
    - 쏠림 공격 : 확률적으로 쏠려있을 경우 그 집단의 정보를 확률적으로 수집 가능
    - 유사성 공격 : 다른 내용이라도 의미가 비슷할 수 있다.
  - t-근접성 (전체 데이터 정보 분포와 특정 정보 분포 차이를 t 이하)
    - 쏠림공격 대비 분포차이 고려
    - 정보의 분포를 조정하여 정보가 특정 값으로 쏠리거나 유사한 값들이 뭉치는 경우를 방지

#### 데이터 품질 검증

- 데이터 품질 관리
  - 정의
  - 중요성
    - 분석 결과의 신뢰성 확보, 일원화된 프로세스, 데이터 활용도 향상, 양질의 데이터 확보
- 데이터 품질
  - 정형데이터 품질 기준
    - 완전성 : 개별, 조건
    - 유일성 : 단독, 조건
    - 유효성 : 범위, 날짜, 형식
    - 일관성 : 기준코드, 참조 무결성, 데이터 흐름, 칼럼
    - 정확성 : 선후 관계, 계산/집계, 최신성, 업무규칙
  - 비정형 데이터 품질 기준
    - 가능성 : 적절성, 정확성, 상호 운용성, 기능 순응성
    - 신뢰성 : 성숙성, 신뢰 순응성
    - 사용성 : 이해성, 친밀성, 사용 순응성
    - 효율성 : 시간, 자원, 효율 순응성
    - 이식성 : 적응성, 공존성, 이식 순응성
- 데이터 품질 진단 기법
  - 정형 데이터
    - 메타 데잍 수집 및 분석
    - 컬럼 속성 분석
    - 누락 값 분석
    - 값의 허용 범위 분석
    - 허용 값 목록 분석
    - 문자열 패턴 분석
    - 날짜 유형 분석
    - 기타 특수 도메인
    - 유일 값 분석
    - 구조 분석
  - 비정형 데이터 품질 진단 (위 품질 기준)
    - 기능성
      - 부가요소 정확성
      - 운용 적절성
      - 사운드 자막 동기화
      - 규격화 여부
    - 신뢰성
      - 결함 발생 정도
      - 규격 준수 정도
    - 사용성
      - 영상 인식 만족도
      - 포멧 친숙성
      - 규격화 여부
    - 효율성
      - 응답 속도
      - 규격화 여부
    - 이식성
      - 운영 환경 호환성
      - 타 SW영향 여부
      - 규격화 여부
- 데이터 품질 검증 수행

### 데이터 적재 및 저장

#### 데이터 적재

- 데이터 적재 도구
  - 데이터 수집 도구를 이용한 데이터 적재
    - 플루언티드
    - 플럼
    - 스크라이브
    - 로그스테시
  - NoSQL, DBMS가 제공하는 도구를 이용한 데이터 적재
  - 관계형 DBMS의 데이터를 NoSQL, DBMS에 적재
- 데이터 적재 완료 테스트
  - 데이터 적재 내용에 따라 체크리스트를 작성
  - 데이터 테스트 케이스를 개발
  - 체크리스트 검증 및 데이터 테스트 케이스 실행

#### 데이터 저장

- 빅데이터 저장시스템
  - 파일 시스템 저장방식
    - HDFS, GFS
  - 데이터베이스 저장방식
    - NoSQL : 수평적 확장성, 데이터 복제, 간편 API, 일관성
- 분산 파일 시스템
  - 하둡 분산파일 시스템 HDFS
    - 여러 블록으로 분산하여 저장. 마지막 제외 모두 동일한 크기
    - HDFS는 마스터 하나와 여러 개의 슬레이브로 클러스터링 되어 구성
      - 마스터 노드는 네임노드라고 하며 슬레이브를 관리하는 메타데이터와 모니터링 시스템 운영
      - 슬레이브노드는 데이터노드라고 하며 데이터 블록을 분산처리
    - 데이터 손상을 방지하기 위해서 복제기법 사용
    - 분산 데이터 처리 기술 MapReduce
  - 구글 파일 시스템 GFS
    - 엄청나게 많은 데이터를 구글의 핵심 데이터 스토리지와 검색 엔진을 위한 최적화된 분산 시스템
    - 마스터, 청크서버, 클라이언트로 구성
      - 마스터는 전체의 상태 관리
      - 청크서버는 물리적인 하드디스크의 실제 입출력 처리
      - 클라이언트는 파일을 읽고 쓰는 동작을 요청하는 에플리케이션
    - 가격이 저렴한 서버에서도 사용되도록 설계되었기 때문에 하드웨어 안정성이나 자료들의 유실에 대해서 고려.
- NoSQL
  - Not Only SQL
  - RDBMS
    - 장점 :
      - 데이터 무결성, 정확성
      - 정규화된 테이블, 소규모 트랜잭션
    - 단점 :
      - 확장성 한계
      - 클라우드 분산 환경에 부적합
  - NoSQL
    - 장점 : 웹 환경의 다양한 정보를 검색, 저장 가능
    - 단점 : 데이터의 무결성과 정확성을 보장하지 않는다
  - CAP이론
    - 일관성
    - 가용성
    - 지속성
  - RDBMS : 일관성, 가용성을 선택
  - NoSQL : 일관성이나 가용성 중 하나를 포기하고 지속성을 보장
  - NoSQL 기술적 특성
    - 무 스키마
    - 탄력성 E;astocotu
    - 질의기능: Query
    - 캐싱
  - NoSQL데이터 모델
    - Key-Value : 아마존 Dynamo, Redis 의 인메모리 방식
      - 가장 간단한 형식
      - 범위 질의는 사용 어렵다
      - 응용 프로그램 모델링 어렵다
    - 열기반 : Cassandra, HBase, HyperTable
      - 연관된 데이터 위주로 읽는데 유리
      - 하나의 레코드를 변경하려면 여러곳 수정 필요
      - 동일 도메인의 열값이 연속되므로 압축 효율이 좋다, 범의 질의 유리
    - 문서기반 : MongoDB
      - 문서마다 다른 스키마가 있다
      - 레코드 간의 관계 설명 가능
      - 개녕적으로 RDBMS 와 유사
- 빅데이터 저장 시스템 선정을 위한 분석
  - 기능성 비교분석
    - 데이터모델
    - 확장성 : 열기반이 가장 좋음
    - 트랜젝션 일관성 : 일관성이 중요하면 RDBMS 를 쓰자.
    - 질의 지원 : MongoDB가 좋다
    - 접근성 : 몽고디비 2승
  - 분석방식 및 환경
  - 분석대상 데이터 유형
  - 기존 시스템과의 연계
- 데이터 발생 유형 및 특성
  - 대용량 실시간 서비스 데이터 개요
  - 대용량 실시간 서비스 데이터 저장
- 안정성과 신뢰성 확보 및 접근성 제어계획 수립
  - 빅데이터 저장 시스템 안정성 및 신뢰성 확보
  - 접근성 제어 계획 수립
